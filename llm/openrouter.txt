"""Manage model-agnostic LLM functions.

author: Peter Karacsonyi
date:   8/15/2025
"""

# stdlib
import re
import os
import json
import time
import requests
from typing import ClassVar
from http import HTTPStatus
from dotenv import dotenv_values

# pypi
from jsonschema import validate, ValidationError, SchemaError
from tenacity import retry, stop_after_attempt, retry_if_exception_type

# local
from llm.model import LLMConfig

# constants
# message to raise within a custom exception
TC_RR_MSG: str = 're-raising exception for tenacity to retry'
# must parse the recommended waiting time from the message out from the openrouter exception
COMPONENT_WAIT_SEC: re.Pattern = re.compile(r'(?P<wait_sec>\d+)\.\d+s')


# Define a custom exception
class RateLimitExceededError(Exception):
    """Custom LLM API RateLimit Error class to raise for tenacity to retry."""

    def __init__(self, message: str) -> None:
        """Instantiate the exception class with a custom message."""
        super().__init__(message)
        self.message = message


class OpenRouter:
    """Abstract away openrouter specific settings."""

    config: ClassVar[dict] = dotenv_values('.env')
    openrouter_api_key: ClassVar[str] = config.get('OPENROUTER_API_KEY', os.getenv('OPENROUTER_API_KEY'))
    url: ClassVar[str] = 'https://openrouter.ai/api/v1/chat/completions'
    headers: ClassVar[dict[str, str]] = {
        'Authorization': f'Bearer {openrouter_api_key}',
        'Content-Type': 'application/json',
    }

    def __init__(self, llmconfig: LLMConfig) -> None:
        """Instantiate an openrouter class with the model-specific llm configuration."""
        self.structured_output = None
        self.llmconfig = llmconfig

        if llmconfig.response_schema_dict:
            self.structured_output = {
                'type': 'json_schema',
                'json_schema': {
                    'name': 'asset_analytics',
                    'strict': True,
                    'schema': llmconfig.response_schema_dict,
                },
            }

    def query(self, prompt_data: str | dict) -> dict:
        """Send query into a large language model.

        prompt data is appended after the superprompt (if set)
        """
        prompt = None
        if self.llmconfig.superprompt_str:
            prompt = str(self.llmconfig.superprompt_str) + '\n' + str(prompt_data)
        else:
            prompt = str(prompt_data)

        if self.llmconfig.response_schema_dict:
            payload = {
                'models': [self.llmconfig.model_name],
                'messages': [{'role': 'user', 'content': prompt}],
                'response_format': self.structured_output,
            }
        else:
            payload = {
                'models': [self.llmconfig.model_name],
                'messages': [{'role': 'user', 'content': json.loads(prompt)}],
            }

        @retry(stop=(stop_after_attempt(6)), retry=retry_if_exception_type(RateLimitExceededError))
        def query_llm(**kwargs: dict) -> requests.Response:
            try:
                response = requests.post(**kwargs, timeout=60)
                if (openrouter_error := response.json().get('choices', [{'error': 0}])[0].get('error', 0)) != 0:
                    raise RateLimitExceededError(
                        openrouter_error.get('message', 'got an empty error message from openrouter, please check')
                    )  # TODO(<peet>): check TRY301
                return response
            except RateLimitExceededError as openrouter_error_message:
                matches = re.search(self.COMPONENT_WAIT_SEC, openrouter_error_message.message)
                time.sleep(int(matches.group('wait_sec')) + 1) if matches else time.sleep(60)
                raise RateLimitExceededError(TC_RR_MSG) from openrouter_error_message

        try:
            response = query_llm(url=self.url, headers=self.headers, json=payload)
        except Exception as e:
            print('unexpected error: ', e)
            return {}

        if response.status_code != HTTPStatus.OK:
            exc_msg = f'Error {response.status_code}: {response.text}'
            raise Exception(exc_msg)
        if (openrouter_error := response.json()['choices'][0].get('error', 0)) != 0:
            raise RateLimitExceededError(
                openrouter_error.get('message', 'got an empty error message from openrouter, please check'),
            )

        # do not fail on validation errors
        try:
            validate(
                instance=json.loads(response.json()['choices'][0]['message']['content']),
                schema=self.llmconfig.response_schema_dict,
            )
        except ValidationError as ve:
            print(f'Validation failed: {ve.message}')
            print(f'Location in instance: {list(ve.path)}')
            print(f'Schema path: {list(ve.schema_path)}')
            if isinstance(ve.instance, str):
                print(f'Invalid value: {ve.instance[:120]}...')
        except SchemaError as se:
            print(f'The schema {self.llmconfig.response_schema_dict} seems to be bad, error: {se.message}')

        return response.json()
